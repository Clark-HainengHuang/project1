{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22c76da",
   "metadata": {},
   "source": [
    "# 14813 final project option1 (fifa data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e540b1",
   "metadata": {},
   "source": [
    "co_work of\n",
    "\n",
    "hainengh,\\\n",
    "Haineng Huang\n",
    "    \n",
    "kaixins,\\\n",
    "Kaixin Song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c3a05",
   "metadata": {},
   "source": [
    "## 1 Docker setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfa6fd",
   "metadata": {},
   "source": [
    "this part has largely referenced to the work of Yiren Zhou <<yirenzho@andrew.cmu.edu>>, and uses the Docker file provided by Yiren Zhou, thanks very much.\\\n",
    "The oringinal work can be visited here: <<https://github.com/ML-Systems-and-Toolchains/course-project-option-1-sample-2>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd265cd",
   "metadata": {},
   "source": [
    "### 1.1 start the contaioner (for both windows and linux system)\n",
    "\n",
    "1.Please ensure that Docker is available (including docker compose).\\\n",
    "2.download the code from github,unzip if needed.For Linux, we suggest to put the folder under /home/\\\n",
    "3.open a cmd window (Windows) or a terminal (Linux)\\\n",
    "4.`cd` to downloaded file location\\\n",
    "5.execute `docker compose up -d`,which will install all the necessary dependencies.\\\n",
    "6.after the container starts, get into workspace by `docker exec -it --user root cp /bin/bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916962a",
   "metadata": {},
   "source": [
    "### 1.2 change user password of PostgreSQL and create schema\n",
    "1.start the postgresql server by `service postgresql start`\\\n",
    "2.get into postgresql workspace by `sudo -u postgres psql`\\\n",
    "3.execute the following two SQL statements to 1) change the password and 2) create a schema named \"fifa\":\\\n",
    "`ALTER USER postgres PASSWORD 'bigdata';`\\\n",
    "`CREATE SCHEMA fifa;`\\\n",
    "4.exit the shell by `\\q`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10527741",
   "metadata": {},
   "source": [
    "### 1.3 open jupyter notebook and execute code for Task 1_2_3\n",
    "1.start the Jupyter notebook within the container by `jupyter notebook --ip 0.0.0.0 --allow-root`\\\n",
    "2.you should see a URL like: `http://127.0.0.1:8888/?token=574bb631fc2e3c790dcf6c0317f9f3ae674cab80264f1707`\\\n",
    "3.copy the URL (better use the one with `127.0.0.1`) and open it in web broswer\\\n",
    "4.Open <Task_1_2_3.ipynb>, and execute the cells sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb097266",
   "metadata": {},
   "source": [
    "### 1.4 Exit and stop container\n",
    "\n",
    "1.exiting the Docker container by `Ctrl + D`\n",
    "\n",
    "2.execute the following to stop the container\n",
    "\n",
    "```\n",
    "docker compose down\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60ec45",
   "metadata": {},
   "source": [
    "## 2 Task-I:Build and populate necessary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcdb56",
   "metadata": {},
   "source": [
    "### 2.1 Build and populate necessary tables\n",
    "implement the code cells in the jupyter notebook for the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4ed088",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:08:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://739fd847af87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GenericAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=GenericAppName>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85cf0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "df_15 = spark.read.csv(\"data/players_15.csv\",header=True, inferSchema= True)\n",
    "df_16 = spark.read.csv(\"data/players_16.csv\",header=True, inferSchema= True)\n",
    "df_17 = spark.read.csv(\"data/players_17.csv\",header=True, inferSchema= True)\n",
    "df_18 = spark.read.csv(\"data/players_18.csv\",header=True, inferSchema= True)\n",
    "df_19 = spark.read.csv(\"data/players_19.csv\",header=True, inferSchema= True)\n",
    "df_20 = spark.read.csv(\"data/players_20.csv\",header=True, inferSchema= True)\n",
    "df_21 = spark.read.csv(\"data/players_21.csv\",header=True, inferSchema= True)\n",
    "df_22 = spark.read.csv(\"data/players_22.csv\",header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01880243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:09:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "-RECORD 0-------------------------------------------\n",
      " sofifa_id                   | 158023               \n",
      " player_url                  | https://sofifa.co... \n",
      " short_name                  | L. Messi             \n",
      " long_name                   | Lionel Andr√©s Mes... \n",
      " player_positions            | CF                   \n",
      " overall                     | 93                   \n",
      " potential                   | 95                   \n",
      " value_eur                   | 1.005E8              \n",
      " wage_eur                    | 550000.0             \n",
      " age                         | 27                   \n",
      " dob                         | 1987-06-24 00:00:00  \n",
      " height_cm                   | 169                  \n",
      " weight_kg                   | 67                   \n",
      " club_team_id                | 241.0                \n",
      " club_name                   | FC Barcelona         \n",
      " league_name                 | Spain Primera Div... \n",
      " league_level                | 1                    \n",
      " club_position               | CF                   \n",
      " club_jersey_number          | 10                   \n",
      " club_loaned_from            | null                 \n",
      " club_joined                 | 2004-07-01 00:00:00  \n",
      " club_contract_valid_until   | 2018                 \n",
      " nationality_id              | 52                   \n",
      " nationality_name            | Argentina            \n",
      " nation_team_id              | 1369.0               \n",
      " nation_position             | CF                   \n",
      " nation_jersey_number        | 10                   \n",
      " preferred_foot              | Left                 \n",
      " weak_foot                   | 3                    \n",
      " skill_moves                 | 4                    \n",
      " international_reputation    | 5                    \n",
      " work_rate                   | Medium/Low           \n",
      " body_type                   | Normal (170-)        \n",
      " real_face                   | Yes                  \n",
      " release_clause_eur          | null                 \n",
      " player_tags                 | #Speedster, #Drib... \n",
      " player_traits               | Finesse Shot, Spe... \n",
      " pace                        | 93                   \n",
      " shooting                    | 89                   \n",
      " passing                     | 86                   \n",
      " dribbling                   | 96                   \n",
      " defending                   | 27                   \n",
      " physic                      | 63                   \n",
      " attacking_crossing          | 84                   \n",
      " attacking_finishing         | 94                   \n",
      " attacking_heading_accuracy  | 71                   \n",
      " attacking_short_passing     | 89                   \n",
      " attacking_volleys           | 85                   \n",
      " skill_dribbling             | 96                   \n",
      " skill_curve                 | 89                   \n",
      " skill_fk_accuracy           | 90                   \n",
      " skill_long_passing          | 76                   \n",
      " skill_ball_control          | 96                   \n",
      " movement_acceleration       | 96                   \n",
      " movement_sprint_speed       | 90                   \n",
      " movement_agility            | 94                   \n",
      " movement_reactions          | 94                   \n",
      " movement_balance            | 95                   \n",
      " power_shot_power            | 80                   \n",
      " power_jumping               | 73                   \n",
      " power_stamina               | 77                   \n",
      " power_strength              | 60                   \n",
      " power_long_shots            | 88                   \n",
      " mentality_aggression        | 48                   \n",
      " mentality_interceptions     | 22                   \n",
      " mentality_positioning       | 92                   \n",
      " mentality_vision            | 90                   \n",
      " mentality_penalties         | 76                   \n",
      " mentality_composure         | null                 \n",
      " defending_marking_awareness | 25                   \n",
      " defending_standing_tackle   | 21                   \n",
      " defending_sliding_tackle    | 20                   \n",
      " goalkeeping_diving          | 6                    \n",
      " goalkeeping_handling        | 11                   \n",
      " goalkeeping_kicking         | 15                   \n",
      " goalkeeping_positioning     | 14                   \n",
      " goalkeeping_reflexes        | 8                    \n",
      " goalkeeping_speed           | null                 \n",
      " ls                          | 89+3                 \n",
      " st                          | 89+3                 \n",
      " rs                          | 89+3                 \n",
      " lw                          | 92+3                 \n",
      " lf                          | 90+3                 \n",
      " cf                          | 90+3                 \n",
      " rf                          | 90+3                 \n",
      " rw                          | 92+3                 \n",
      " lam                         | 92+3                 \n",
      " cam                         | 92+3                 \n",
      " ram                         | 92+3                 \n",
      " lm                          | 90+3                 \n",
      " lcm                         | 79+3                 \n",
      " cm                          | 79+3                 \n",
      " rcm                         | 79+3                 \n",
      " rm                          | 90+3                 \n",
      " lwb                         | 62+3                 \n",
      " ldm                         | 62+3                 \n",
      " cdm                         | 62+3                 \n",
      " rdm                         | 62+3                 \n",
      " rwb                         | 62+3                 \n",
      " lb                          | 54+3                 \n",
      " lcb                         | 45+3                 \n",
      " cb                          | 45+3                 \n",
      " rcb                         | 45+3                 \n",
      " rb                          | 54+3                 \n",
      " gk                          | 15+3                 \n",
      " player_face_url             | https://cdn.sofif... \n",
      " club_logo_url               | https://cdn.sofif... \n",
      " club_flag_url               | https://cdn.sofif... \n",
      " nation_logo_url             | https://cdn.sofif... \n",
      " nation_flag_url             | https://cdn.sofif... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_15.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9cbec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sofifa_id: integer (nullable = true)\n",
      " |-- player_url: string (nullable = true)\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- long_name: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: integer (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: integer (nullable = true)\n",
      " |-- club_loaned_from: string (nullable = true)\n",
      " |-- club_joined: timestamp (nullable = true)\n",
      " |-- club_contract_valid_until: integer (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- nation_team_id: double (nullable = true)\n",
      " |-- nation_position: string (nullable = true)\n",
      " |-- nation_jersey_number: integer (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- real_face: string (nullable = true)\n",
      " |-- release_clause_eur: string (nullable = true)\n",
      " |-- player_tags: string (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: string (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- goalkeeping_speed: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- player_face_url: string (nullable = true)\n",
      " |-- club_logo_url: string (nullable = true)\n",
      " |-- club_flag_url: string (nullable = true)\n",
      " |-- nation_logo_url: string (nullable = true)\n",
      " |-- nation_flag_url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_15.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a442dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "df_15 = df_15.withColumn(\"year\",functions.lit(2015))\n",
    "df_16 = df_16.withColumn(\"year\",functions.lit(2016))\n",
    "df_17 = df_17.withColumn(\"year\",functions.lit(2017))\n",
    "df_18 = df_18.withColumn(\"year\",functions.lit(2018))\n",
    "df_19 = df_19.withColumn(\"year\",functions.lit(2019))\n",
    "df_20 = df_20.withColumn(\"year\",functions.lit(2020))\n",
    "df_21 = df_21.withColumn(\"year\",functions.lit(2021))\n",
    "df_22 = df_22.withColumn(\"year\",functions.lit(2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4296077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df count is 16155\n",
      "df count is 15623\n",
      "df count is 17596\n",
      "df count is 17954\n",
      "df count is 18085\n",
      "df count is 18483\n",
      "df count is 18944\n",
      "df count is 19239\n",
      "united df count is 142079\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def unite_dfs(df1, df2):\n",
    "    print(\"df count is\",df2.count())\n",
    "    return df2.union(df1)\n",
    "\n",
    "list_of_dfs = [df_15, df_16, df_17, df_18, df_19, df_20, df_21, df_22]\n",
    "print(\"df count is\",df_15.count())\n",
    "united_df = reduce(unite_dfs, list_of_dfs)\n",
    "print(\"united df count is\",united_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dcc3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id,lit\n",
    "df = united_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b5911",
   "metadata": {},
   "source": [
    "add a new column 'id' for the tabel, which is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9353929",
   "metadata": {},
   "source": [
    "## 2.2 write the pyspark dataframe to postgresql table\n",
    "implement the code cells in the jupyter notebook for the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08000d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"postgres\"\n",
    "#update your db password\n",
    "db_properties['password']=\"bigdata\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres?currentSchema=fifa\"\n",
    "#db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"fifa\"\n",
    "\n",
    "\n",
    "df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbcf95f",
   "metadata": {},
   "source": [
    "## 2.3 setting the constraint for table\n",
    "(for both windows & linux)\\\n",
    "1.`cd` to docker file location\\\n",
    "2.get into workspace by `docker exec -it --user root cp /bin/bash`\\\n",
    "3.get into postgresql workspace by `sudo -u postgres psql`\\\n",
    "3.setting the constraint for table by following code\\\n",
    "```\n",
    "ALTER TABLE fifa.fifa\n",
    "  ADD CONSTRAINT tablename_pkey \n",
    "     PRIMARY KEY (id); \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26f448",
   "metadata": {},
   "source": [
    "## 2.4 (Attachment) Descriptions for features\n",
    "\n",
    " 'sofifa_id',\n",
    "     id of players in fifa\n",
    "     \n",
    " 'player_url',\n",
    " 'short_name',\n",
    " 'long_name',\n",
    "     name and personal website url\n",
    "     \n",
    " 'player_positions',\n",
    " 'overall',\n",
    " 'potential',\n",
    " 'value_eur',\n",
    " 'wage_eur',\n",
    " 'age',\n",
    " 'dob',\n",
    " 'height_cm',\n",
    " 'weight_kg',\n",
    "     basical informations for players \n",
    "     \n",
    " 'club_team_id',\n",
    " 'club_name',\n",
    " 'league_name',\n",
    " 'league_level',\n",
    " 'club_position',\n",
    " 'club_jersey_number',\n",
    " 'club_loaned_from',\n",
    " 'club_joined',\n",
    " 'club_contract_valid_until',\n",
    " 'nationality_id',\n",
    " 'nationality_name',\n",
    " 'nation_team_id',\n",
    " 'nation_position',\n",
    " 'nation_jersey_number',\n",
    "     club and nation information\n",
    "     \n",
    " 'preferred_foot',\n",
    " 'weak_foot',\n",
    " 'skill_moves',\n",
    " 'international_reputation',\n",
    " 'work_rate',\n",
    " 'body_type',\n",
    " 'real_face',\n",
    " 'release_clause_eur',\n",
    "     information for personal \n",
    "     \n",
    " 'player_tags',\n",
    "     tag on players\n",
    "     \n",
    " 'player_traits',\n",
    "     special traits of players\n",
    "     \n",
    " 'pace',\n",
    " 'shooting',\n",
    " 'passing',\n",
    " 'dribbling',\n",
    " 'defending',\n",
    " 'physic',\n",
    " 'attacking_crossing',\n",
    " 'attacking_finishing',\n",
    " 'attacking_heading_accuracy',\n",
    " 'attacking_short_passing',\n",
    " 'attacking_volleys',\n",
    " 'skill_dribbling',\n",
    " 'skill_curve',\n",
    " 'skill_fk_accuracy',\n",
    " 'skill_long_passing',\n",
    " 'skill_ball_control',\n",
    " 'movement_acceleration',\n",
    " 'movement_sprint_speed',\n",
    " 'movement_agility',\n",
    " 'movement_reactions',\n",
    " 'movement_balance',\n",
    " 'power_shot_power',\n",
    " 'power_jumping',\n",
    " 'power_stamina',\n",
    " 'power_strength',\n",
    " 'power_long_shots',\n",
    " 'mentality_aggression',\n",
    " 'mentality_interceptions',\n",
    " 'mentality_positioning',\n",
    " 'mentality_vision',\n",
    " 'mentality_penalties',\n",
    " 'mentality_composure',\n",
    " 'defending_marking_awareness',\n",
    " 'defending_standing_tackle',\n",
    " 'defending_sliding_tackle',\n",
    " 'goalkeeping_diving',\n",
    " 'goalkeeping_handling',\n",
    " 'goalkeeping_kicking',\n",
    " 'goalkeeping_positioning',\n",
    " 'goalkeeping_reflexes',\n",
    " 'goalkeeping_speed',\n",
    "     score/grades on performance\n",
    "     \n",
    " 'ls',\n",
    " 'st',\n",
    " 'rs',\n",
    " 'lw',\n",
    " 'lf',\n",
    " 'cf',\n",
    " 'rf',\n",
    " 'rw',\n",
    " 'lam',\n",
    " 'cam',\n",
    " 'ram',\n",
    " 'lm',\n",
    " 'lcm',\n",
    " 'cm',\n",
    " 'rcm',\n",
    " 'rm',\n",
    " 'lwb',\n",
    " 'ldm',\n",
    " 'cdm',\n",
    " 'rdm',\n",
    " 'rwb',\n",
    " 'lb',\n",
    " 'lcb',\n",
    " 'cb',\n",
    " 'rcb',\n",
    " 'rb',\n",
    " 'gk',\n",
    "     score/grades for different positions on the playground\n",
    "     \n",
    " 'player_face_url',\n",
    " 'club_logo_url',\n",
    " 'club_flag_url',\n",
    " 'nation_logo_url',\n",
    " 'nation_flag_url',\n",
    "     picture url for players and their club, nation\n",
    "     \n",
    " 'year'\n",
    "     the year of csv files this row of data came from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9af0a",
   "metadata": {},
   "source": [
    "## 3 Task-II: Conduct analytics on your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60e944",
   "metadata": {},
   "source": [
    "### 3.1 read the data from postgresql \n",
    "implement the code cells in the jupyter notebook for the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9eaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c81e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07123ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142079"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6b54c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sofifa_id: integer (nullable = true)\n",
      " |-- player_url: string (nullable = true)\n",
      " |-- short_name: string (nullable = true)\n",
      " |-- long_name: string (nullable = true)\n",
      " |-- player_positions: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- potential: integer (nullable = true)\n",
      " |-- value_eur: double (nullable = true)\n",
      " |-- wage_eur: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      " |-- height_cm: integer (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- club_team_id: double (nullable = true)\n",
      " |-- club_name: string (nullable = true)\n",
      " |-- league_name: string (nullable = true)\n",
      " |-- league_level: integer (nullable = true)\n",
      " |-- club_position: string (nullable = true)\n",
      " |-- club_jersey_number: integer (nullable = true)\n",
      " |-- club_loaned_from: string (nullable = true)\n",
      " |-- club_joined: timestamp (nullable = true)\n",
      " |-- club_contract_valid_until: integer (nullable = true)\n",
      " |-- nationality_id: integer (nullable = true)\n",
      " |-- nationality_name: string (nullable = true)\n",
      " |-- nation_team_id: double (nullable = true)\n",
      " |-- nation_position: string (nullable = true)\n",
      " |-- nation_jersey_number: integer (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- weak_foot: integer (nullable = true)\n",
      " |-- skill_moves: integer (nullable = true)\n",
      " |-- international_reputation: integer (nullable = true)\n",
      " |-- work_rate: string (nullable = true)\n",
      " |-- body_type: string (nullable = true)\n",
      " |-- real_face: string (nullable = true)\n",
      " |-- release_clause_eur: string (nullable = true)\n",
      " |-- player_tags: string (nullable = true)\n",
      " |-- player_traits: string (nullable = true)\n",
      " |-- pace: integer (nullable = true)\n",
      " |-- shooting: integer (nullable = true)\n",
      " |-- passing: integer (nullable = true)\n",
      " |-- dribbling: integer (nullable = true)\n",
      " |-- defending: integer (nullable = true)\n",
      " |-- physic: integer (nullable = true)\n",
      " |-- attacking_crossing: integer (nullable = true)\n",
      " |-- attacking_finishing: integer (nullable = true)\n",
      " |-- attacking_heading_accuracy: integer (nullable = true)\n",
      " |-- attacking_short_passing: integer (nullable = true)\n",
      " |-- attacking_volleys: integer (nullable = true)\n",
      " |-- skill_dribbling: integer (nullable = true)\n",
      " |-- skill_curve: integer (nullable = true)\n",
      " |-- skill_fk_accuracy: integer (nullable = true)\n",
      " |-- skill_long_passing: integer (nullable = true)\n",
      " |-- skill_ball_control: integer (nullable = true)\n",
      " |-- movement_acceleration: integer (nullable = true)\n",
      " |-- movement_sprint_speed: integer (nullable = true)\n",
      " |-- movement_agility: integer (nullable = true)\n",
      " |-- movement_reactions: integer (nullable = true)\n",
      " |-- movement_balance: integer (nullable = true)\n",
      " |-- power_shot_power: integer (nullable = true)\n",
      " |-- power_jumping: integer (nullable = true)\n",
      " |-- power_stamina: integer (nullable = true)\n",
      " |-- power_strength: integer (nullable = true)\n",
      " |-- power_long_shots: integer (nullable = true)\n",
      " |-- mentality_aggression: integer (nullable = true)\n",
      " |-- mentality_interceptions: integer (nullable = true)\n",
      " |-- mentality_positioning: integer (nullable = true)\n",
      " |-- mentality_vision: integer (nullable = true)\n",
      " |-- mentality_penalties: integer (nullable = true)\n",
      " |-- mentality_composure: string (nullable = true)\n",
      " |-- defending_marking_awareness: integer (nullable = true)\n",
      " |-- defending_standing_tackle: integer (nullable = true)\n",
      " |-- defending_sliding_tackle: integer (nullable = true)\n",
      " |-- goalkeeping_diving: integer (nullable = true)\n",
      " |-- goalkeeping_handling: integer (nullable = true)\n",
      " |-- goalkeeping_kicking: integer (nullable = true)\n",
      " |-- goalkeeping_positioning: integer (nullable = true)\n",
      " |-- goalkeeping_reflexes: integer (nullable = true)\n",
      " |-- goalkeeping_speed: integer (nullable = true)\n",
      " |-- ls: string (nullable = true)\n",
      " |-- st: string (nullable = true)\n",
      " |-- rs: string (nullable = true)\n",
      " |-- lw: string (nullable = true)\n",
      " |-- lf: string (nullable = true)\n",
      " |-- cf: string (nullable = true)\n",
      " |-- rf: string (nullable = true)\n",
      " |-- rw: string (nullable = true)\n",
      " |-- lam: string (nullable = true)\n",
      " |-- cam: string (nullable = true)\n",
      " |-- ram: string (nullable = true)\n",
      " |-- lm: string (nullable = true)\n",
      " |-- lcm: string (nullable = true)\n",
      " |-- cm: string (nullable = true)\n",
      " |-- rcm: string (nullable = true)\n",
      " |-- rm: string (nullable = true)\n",
      " |-- lwb: string (nullable = true)\n",
      " |-- ldm: string (nullable = true)\n",
      " |-- cdm: string (nullable = true)\n",
      " |-- rdm: string (nullable = true)\n",
      " |-- rwb: string (nullable = true)\n",
      " |-- lb: string (nullable = true)\n",
      " |-- lcb: string (nullable = true)\n",
      " |-- cb: string (nullable = true)\n",
      " |-- rcb: string (nullable = true)\n",
      " |-- rb: string (nullable = true)\n",
      " |-- gk: string (nullable = true)\n",
      " |-- player_face_url: string (nullable = true)\n",
      " |-- club_logo_url: string (nullable = true)\n",
      " |-- club_flag_url: string (nullable = true)\n",
      " |-- nation_logo_url: string (nullable = true)\n",
      " |-- nation_flag_url: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840e4cb",
   "metadata": {},
   "source": [
    "### 3.2 Functions for analyze\n",
    "implement the code cells in the jupyter notebook for the step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fca0f5",
   "metadata": {},
   "source": [
    "3.2.1 What are the X=5 clubs that have the highest number of players with contracts ending in 2023 (in 2022 data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d42bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "def find_top_clabs_contrasts_ending_2023(df,X):\n",
    "    # 2022 data only\n",
    "    df_22 = df.filter(df.year ==2022 )\n",
    "    df_filterd = df_22.filter(df_22.club_name != 'None').groupBy(\"club_name\").count().sort(F.col('count').desc()).head(5)\n",
    "    df_new = spark.createDataFrame(df_filterd)\n",
    "    #return df_filterd\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cfce4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           club_name|count|\n",
      "+--------------------+-----+\n",
      "|RCD Espanyol de B...|   33|\n",
      "|             Burnley|   33|\n",
      "|    Newcastle United|   33|\n",
      "|    RC Celta de Vigo|   33|\n",
      "| Paris Saint-Germain|   33|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_top_clabs_contrasts_ending_2023(df,5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e45c3",
   "metadata": {},
   "source": [
    "3.2.2 List the Y=5 clubs with highest average number of players that are older than 27 years across all years (i.e. calculate the number of players older than 27 years old for each club in each dataset, calculate the averages and list the Y clubs with highest averages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da1b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age\n",
    "\n",
    "def find_top_clabs_age_over_27(df,Y):\n",
    "    df_age27 = df.filter(df.age > 27 )\n",
    "    df_filtered = df_age27.groupBy(\"club_name\").mean(\"age\").sort(F.col(\"avg(age)\").desc()).head(Y)\n",
    "    #df_filtered = df_age27.groupBy(\"club_name\").mean(\"age\")\n",
    "    df_new = spark.createDataFrame(df_filtered)\n",
    "    #return df_filtered\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e24531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           club_name|          avg(age)|\n",
      "+--------------------+------------------+\n",
      "|         Yokohama FC|  34.7037037037037|\n",
      "|      Wexford Youths|              34.0|\n",
      "|  Zamora F√∫tbol Club|33.857142857142854|\n",
      "|Centro Atl√©tico F...|              33.6|\n",
      "|      CF Fuenlabrada| 33.54545454545455|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_top_clabs_age_over_27(df,5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe7abe",
   "metadata": {},
   "source": [
    "3.2.3 What is the most frequent nation_position in the dataset for each year? (i.e.display the most frequent nation_position for 2015, 2016, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75861146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nation_position\n",
    "\n",
    "def find_most_nation_from2015to2022(df):\n",
    "    years = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"]\n",
    "    for y in years:\n",
    "        df_year = df.filter(df.year == y)\n",
    "        df_filtered = df_year.filter(df_year.nation_position != 'None').groupBy(\"nation_position\").count().sort(F.col(\"count\").desc()).head(1)\n",
    "        print(\"year of data is \",y)\n",
    "        print(spark.createDataFrame(df_filtered).show())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e8d7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year of data is  2015\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  564|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2016\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  511|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2017\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  564|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2018\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  600|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2019\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  576|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2020\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  588|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2021\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  588|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n",
      "year of data is  2022\n",
      "+---------------+-----+\n",
      "|nation_position|count|\n",
      "+---------------+-----+\n",
      "|            SUB|  396|\n",
      "+---------------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "find_most_nation_from2015to2022(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c169f0",
   "metadata": {},
   "source": [
    "## 4 Task-III: Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3f6be",
   "metadata": {},
   "source": [
    "### 4.1 read the data from postgresql \n",
    "implement the code cells in the jupyter notebook for the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65aa0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be07cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 108:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------\n",
      " sofifa_id                   | 244554               \n",
      " player_url                  | https://sofifa.co... \n",
      " short_name                  | J. Dodd              \n",
      " long_name                   | James Dodd           \n",
      " player_positions            | CM                   \n",
      " overall                     | 53                   \n",
      " potential                   | 66                   \n",
      " value_eur                   | 100000.0             \n",
      " wage_eur                    | 700.0                \n",
      " age                         | 18                   \n",
      " dob                         | 2000-10-27 00:00:00  \n",
      " height_cm                   | 178                  \n",
      " weight_kg                   | 66                   \n",
      " club_team_id                | 143.0                \n",
      " club_name                   | Exeter City          \n",
      " league_name                 | English League Two   \n",
      " league_level                | 4                    \n",
      " club_position               | RES                  \n",
      " club_jersey_number          | 36                   \n",
      " club_loaned_from            | null                 \n",
      " club_joined                 | 2018-07-01 00:00:00  \n",
      " club_contract_valid_until   | 2020                 \n",
      " nationality_id              | 14                   \n",
      " nationality_name            | England              \n",
      " nation_team_id              | null                 \n",
      " nation_position             | null                 \n",
      " nation_jersey_number        | null                 \n",
      " preferred_foot              | Right                \n",
      " weak_foot                   | 3                    \n",
      " skill_moves                 | 2                    \n",
      " international_reputation    | 1                    \n",
      " work_rate                   | Medium/Medium        \n",
      " body_type                   | Lean (170-185)       \n",
      " real_face                   | No                   \n",
      " release_clause_eur          | 238000               \n",
      " player_tags                 | null                 \n",
      " player_traits               | null                 \n",
      " pace                        | 69                   \n",
      " shooting                    | 42                   \n",
      " passing                     | 50                   \n",
      " dribbling                   | 55                   \n",
      " defending                   | 42                   \n",
      " physic                      | 51                   \n",
      " attacking_crossing          | 42                   \n",
      " attacking_finishing         | 41                   \n",
      " attacking_heading_accuracy  | 40                   \n",
      " attacking_short_passing     | 55                   \n",
      " attacking_volleys           | 37                   \n",
      " skill_dribbling             | 51                   \n",
      " skill_curve                 | 43                   \n",
      " skill_fk_accuracy           | 35                   \n",
      " skill_long_passing          | 54                   \n",
      " skill_ball_control          | 54                   \n",
      " movement_acceleration       | 68                   \n",
      " movement_sprint_speed       | 69                   \n",
      " movement_agility            | 71                   \n",
      " movement_reactions          | 51                   \n",
      " movement_balance            | 75                   \n",
      " power_shot_power            | 46                   \n",
      " power_jumping               | 71                   \n",
      " power_stamina               | 67                   \n",
      " power_strength              | 46                   \n",
      " power_long_shots            | 38                   \n",
      " mentality_aggression        | 36                   \n",
      " mentality_interceptions     | 40                   \n",
      " mentality_positioning       | 48                   \n",
      " mentality_vision            | 53                   \n",
      " mentality_penalties         | 49                   \n",
      " mentality_composure         | 56                   \n",
      " defending_marking_awareness | 41                   \n",
      " defending_standing_tackle   | 45                   \n",
      " defending_sliding_tackle    | 43                   \n",
      " goalkeeping_diving          | 14                   \n",
      " goalkeeping_handling        | 14                   \n",
      " goalkeeping_kicking         | 7                    \n",
      " goalkeeping_positioning     | 13                   \n",
      " goalkeeping_reflexes        | 12                   \n",
      " goalkeeping_speed           | null                 \n",
      " ls                          | 48+2                 \n",
      " st                          | 48+2                 \n",
      " rs                          | 48+2                 \n",
      " lw                          | 52                   \n",
      " lf                          | 51                   \n",
      " cf                          | 51                   \n",
      " rf                          | 51                   \n",
      " rw                          | 52                   \n",
      " lam                         | 53+2                 \n",
      " cam                         | 53+2                 \n",
      " ram                         | 53+2                 \n",
      " lm                          | 53+2                 \n",
      " lcm                         | 52+2                 \n",
      " cm                          | 52+2                 \n",
      " rcm                         | 52+2                 \n",
      " rm                          | 53+2                 \n",
      " lwb                         | 51+2                 \n",
      " ldm                         | 49+2                 \n",
      " cdm                         | 49+2                 \n",
      " rdm                         | 49+2                 \n",
      " rwb                         | 51+2                 \n",
      " lb                          | 50+2                 \n",
      " lcb                         | 45+2                 \n",
      " cb                          | 45+2                 \n",
      " rcb                         | 45+2                 \n",
      " rb                          | 50+2                 \n",
      " gk                          | 17+2                 \n",
      " player_face_url             | https://cdn.sofif... \n",
      " club_logo_url               | https://cdn.sofif... \n",
      " club_flag_url               | https://cdn.sofif... \n",
      " nation_logo_url             | null                 \n",
      " nation_flag_url             | https://cdn.sofif... \n",
      " year                        | 2020                 \n",
      " id                          | 94489280512          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04c27621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the csv if you need\n",
    "#df.repartition(1).write.csv(\"./fifa.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4db4d8",
   "metadata": {},
   "source": [
    "### 4.2 Data Engineering\n",
    "#### Drop columns \n",
    "the following descriptions referenced to the work of Yiren Zhou <<yirenzho@andrew.cmu.edu>>, thanks very much.\\\n",
    "https://github.com/ML-Systems-and-Toolchains/course-project-option-1-sample-2\n",
    "\n",
    "I have decided to drop the following columns as they don't seem relevant to the overall value of a player:\n",
    "\n",
    "player_url (not relevant)\\\n",
    "short_name (not relevant)\\\n",
    "long_name (not relevant) \\\n",
    "club_name (too many values, could be useless) \\\n",
    "club_loaned_from (too few values) \\\n",
    "club_joined(not relevant)\\\n",
    "dob(not relevant)\\\n",
    "release_clause_eur (too few values) \\\n",
    "player_positions (cannot be converted into numerics easily) \\\n",
    "goalkeeping_speed (too few values) \\\n",
    "player_traits (too random) \\\n",
    "player_tags (there are missing entries for certain players)\\\n",
    "real_face (not relevant) \\\n",
    "mentality_composure (too few values) \\\n",
    "player_face_url (not relevant) \\\n",
    "club_logo_url (not relevant) \\\n",
    "club_flag_url (not relevant) \\\n",
    "nation_logo_url (not relevant)\\\n",
    "nation_flag_url (not relevant)\\\n",
    "ls (not relevant)\\\n",
    ".\\\n",
    ".\\\n",
    ".\\\n",
    "gk(not relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f0500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset.withColumn(\"outcome\", col(\"overall\").cast(DoubleType()))\n",
    "        output_df = output_df.drop('overall')\n",
    "        return output_df\n",
    "    \n",
    "class NULLDropper(Transformer): # Drop all rows with NULL values.\n",
    "    def __init__(self,cols):\n",
    "        super().__init__()\n",
    "        self.cols=cols\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        out_df = dataset\n",
    "        return out_df.na.drop(subset=self.cols)\n",
    "\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline(df):\n",
    "    \n",
    "    col_names = df.columns\n",
    "        \n",
    "    # Drop Null columns in what we want\n",
    "    nominal_cols = [\"preferred_foot\", \"body_type\", \"league_name\", \"club_position\", \"nationality_name\", \"nation_position\", \"work_rate\"]\n",
    "    stage_dropnull = NULLDropper(nominal_cols)\n",
    "    \n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector \n",
    "    # (and dropping a few)\n",
    "    feature_cols = col_names + nominal_onehot_cols\n",
    "    corelated_cols_to_remove = ['player_url', 'player_positions', 'club_name', 'short_name', 'long_name', \n",
    "                                'club_loaned_from', 'club_joined', 'dob', 'release_clause_eur', 'goalkeeping_speed', \n",
    "                                'player_traits', 'player_tags', 'real_face', 'mentality_composure', 'player_face_url', \n",
    "                                'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                                    'ls','st','rs','lw','lf','cf','rf','rw','lam','cam',\n",
    "                                    'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm',\n",
    "                                    'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'\n",
    "                               ]\n",
    "    for col_name in nominal_cols + corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_dropnull_afterfeature = NULLDropper(feature_cols)    \n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = col_names+nominal_id_cols+nominal_onehot_cols+['vectorized_features','year'])\n",
    "    \n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_dropnull,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_dropnull_afterfeature,stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32cea3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- outcome: double (nullable = true)\n",
      "\n",
      "+--------------------+-------+\n",
      "|            features|outcome|\n",
      "+--------------------+-------+\n",
      "|(232,[0,1,2,3,4,5...|   53.0|\n",
      "+--------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_preprocess_pipeline(df)\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_transformed = pipeline_model.transform(df)\n",
    "\n",
    "\n",
    "# As you see, only column features and outcome are left.\n",
    "df_transformed.printSchema()\n",
    "df_transformed.show(1)\n",
    "\n",
    "# Train/test split\n",
    "df_train, df_test = df_transformed.randomSplit(weights=[0.8,0.2], seed=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd322beb",
   "metadata": {},
   "source": [
    "### 4.3 model on spark\n",
    "\n",
    "I choosed LinearRegression and RandomForestRegressor.\n",
    "\n",
    "since it's a regression task, I choose MSE(mean squared error) as the target.\n",
    "\n",
    "the dataset is devided to train(80%),test(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e62acf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='outcome', metricName='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8db27f",
   "metadata": {},
   "source": [
    "#### 4.3.1 (spark) LinearRegression\n",
    "\n",
    "\"regParam\" is regularization parameter, which influence the regularization process and thus contributes to model performance. The default value is '0.0'\n",
    "\n",
    "\"maxIter\" is number of iterations, which determines the training times and convergence of the model. The defualt value is '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd786fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 116:>  (0 + 1) / 1][Stage 117:>  (0 + 1) / 1][Stage 118:>  (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:11:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/11/29 04:11:45 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/11/29 04:11:45 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol='features',labelCol='outcome')\n",
    "\n",
    "lr_param_grid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.001,0.01,0.1])\n",
    "                .addGrid(lr.maxIter, [50,100,150])\n",
    "                .build())\n",
    "                 \n",
    "lr_cv = CrossValidator(estimator=lr, \n",
    "                       estimatorParamMaps=lr_param_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       parallelism=8,\n",
    "                       numFolds=5)\n",
    "                 \n",
    "lr_cv_model = lr_cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28b0d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# after Cross-Validation\n",
      "Parameter: Mean Squard Error(MSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE of LinearRegression Model : 2.428514255680475e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 254:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of LinearRegression Model : 2.4940901458240294e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# after Cross-Validation\n",
    "\n",
    "print(\"# after Cross-Validation\")\n",
    "\n",
    "print(\"Parameter: Mean Squard Error(MSE)\")\n",
    "\n",
    "lr_train_predictions = lr_cv_model.transform(df_train)\n",
    "print('Train MSE of LinearRegression Model :', evaluator.evaluate(lr_train_predictions))\n",
    "\n",
    "lr_test_predictions = lr_cv_model.transform(df_test)\n",
    "print('Test MSE of LinearRegression Model :', evaluator.evaluate(lr_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb639cf0",
   "metadata": {},
   "source": [
    "#### 4.3.2 (spark) RandomForestRegressor\n",
    "\"maxDepth\" means the maxmum depth of trees in the RF model, the default value is 4\n",
    "\n",
    "\"numTrees\" means the number of trees in the RF model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64e1d6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/spark-core_2.12-3.3.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:12:30 WARN DAGScheduler: Broadcasting large task binary with size 1096.9 KiB\n",
      "22/11/29 04:12:30 WARN DAGScheduler: Broadcasting large task binary with size 1088.4 KiB\n",
      "22/11/29 04:12:30 WARN DAGScheduler: Broadcasting large task binary with size 1088.4 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1322.6 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1322.6 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1403.0 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1598.6 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1724.6 KiB\n",
      "22/11/29 04:12:31 WARN DAGScheduler: Broadcasting large task binary with size 1724.6 KiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 1649.6 KiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 1649.6 KiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 1802.6 KiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 2029.8 KiB\n",
      "22/11/29 04:12:32 WARN DAGScheduler: Broadcasting large task binary with size 2029.8 KiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1879.0 KiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1096.9 KiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1918.9 KiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:33 WARN DAGScheduler: Broadcasting large task binary with size 1849.8 KiB\n",
      "22/11/29 04:12:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:34 WARN DAGScheduler: Broadcasting large task binary with size 1586.3 KiB\n",
      "22/11/29 04:12:34 WARN DAGScheduler: Broadcasting large task binary with size 1598.6 KiB\n",
      "22/11/29 04:12:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:34 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:12:35 WARN DAGScheduler: Broadcasting large task binary with size 1764.4 KiB\n",
      "22/11/29 04:12:35 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/11/29 04:12:36 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:36 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:12:37 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:12:37 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:12:38 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:12:38 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 480:>  (0 + 1) / 1][Stage 481:>  (0 + 1) / 1][Stage 482:>  (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:12:40 WARN BlockManager: Block rdd_2526_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:12:41 WARN DAGScheduler: Broadcasting large task binary with size 1078.1 KiB\n",
      "22/11/29 04:12:41 WARN DAGScheduler: Broadcasting large task binary with size 1078.1 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1096.6 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1323.5 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1323.5 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1372.0 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1372.0 KiB\n",
      "22/11/29 04:12:42 WARN DAGScheduler: Broadcasting large task binary with size 1601.0 KiB\n",
      "22/11/29 04:12:43 WARN DAGScheduler: Broadcasting large task binary with size 1606.8 KiB\n",
      "22/11/29 04:12:43 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n",
      "22/11/29 04:12:43 WARN DAGScheduler: Broadcasting large task binary with size 1606.8 KiB\n",
      "22/11/29 04:12:43 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n",
      "22/11/29 04:12:43 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1771.7 KiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1855.9 KiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1096.6 KiB\n",
      "22/11/29 04:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1880.1 KiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1891.2 KiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1039.0 KiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1601.0 KiB\n",
      "22/11/29 04:12:45 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:46 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/11/29 04:12:46 WARN DAGScheduler: Broadcasting large task binary with size 2016.8 KiB\n",
      "22/11/29 04:12:47 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:48 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:12:48 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:12:49 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:12:49 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:12:49 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1082.1 KiB\n",
      "22/11/29 04:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1075.7 KiB\n",
      "22/11/29 04:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1075.7 KiB\n",
      "22/11/29 04:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1335.4 KiB\n",
      "22/11/29 04:12:53 WARN DAGScheduler: Broadcasting large task binary with size 1335.4 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1365.6 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1365.6 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1575.0 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1744.5 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1744.5 KiB\n",
      "22/11/29 04:12:54 WARN DAGScheduler: Broadcasting large task binary with size 1601.6 KiB\n",
      "22/11/29 04:12:55 WARN DAGScheduler: Broadcasting large task binary with size 1601.6 KiB\n",
      "22/11/29 04:12:55 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:55 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:55 WARN DAGScheduler: Broadcasting large task binary with size 1736.9 KiB\n",
      "22/11/29 04:12:55 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1801.7 KiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1082.1 KiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1832.5 KiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1846.8 KiB\n",
      "22/11/29 04:12:56 WARN DAGScheduler: Broadcasting large task binary with size 1511.9 KiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 1575.0 KiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 1817.6 KiB\n",
      "22/11/29 04:12:57 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:12:58 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:12:59 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:13:00 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:13:00 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:13:00 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:13:01 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1083.4 KiB\n",
      "22/11/29 04:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1076.1 KiB\n",
      "22/11/29 04:13:04 WARN DAGScheduler: Broadcasting large task binary with size 1076.1 KiB\n",
      "22/11/29 04:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1332.9 KiB\n",
      "22/11/29 04:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1332.9 KiB\n",
      "22/11/29 04:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1380.6 KiB\n",
      "22/11/29 04:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1380.6 KiB\n",
      "22/11/29 04:13:05 WARN DAGScheduler: Broadcasting large task binary with size 1583.1 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 1729.5 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 1729.5 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 1618.9 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 1618.9 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 1757.4 KiB\n",
      "22/11/29 04:13:06 WARN DAGScheduler: Broadcasting large task binary with size 2047.2 KiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 2047.2 KiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1819.8 KiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1774.3 KiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1083.4 KiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:13:07 WARN DAGScheduler: Broadcasting large task binary with size 1690.1 KiB\n",
      "22/11/29 04:13:08 WARN DAGScheduler: Broadcasting large task binary with size 1288.4 KiB\n",
      "22/11/29 04:13:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:13:08 WARN DAGScheduler: Broadcasting large task binary with size 1583.1 KiB\n",
      "22/11/29 04:13:08 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:13:08 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:13:09 WARN DAGScheduler: Broadcasting large task binary with size 1778.7 KiB\n",
      "22/11/29 04:13:09 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:13:10 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:13:10 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:13:11 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:13:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:13:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/11/29 04:13:12 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1065.4 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1090.7 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1065.4 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1303.2 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1303.2 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1360.0 KiB\n",
      "22/11/29 04:13:16 WARN DAGScheduler: Broadcasting large task binary with size 1360.0 KiB\n",
      "22/11/29 04:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1584.9 KiB\n",
      "22/11/29 04:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1696.5 KiB\n",
      "22/11/29 04:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1696.5 KiB\n",
      "22/11/29 04:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1602.7 KiB\n",
      "22/11/29 04:13:17 WARN DAGScheduler: Broadcasting large task binary with size 1602.7 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1753.7 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 2004.9 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 2004.9 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1824.8 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1863.5 KiB\n",
      "22/11/29 04:13:18 WARN DAGScheduler: Broadcasting large task binary with size 1090.7 KiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 1792.7 KiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 1797.8 KiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 1584.9 KiB\n",
      "22/11/29 04:13:19 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/11/29 04:13:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:13:20 WARN DAGScheduler: Broadcasting large task binary with size 1767.4 KiB\n",
      "22/11/29 04:13:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/11/29 04:13:21 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/11/29 04:13:22 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:13:22 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:13:23 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/11/29 04:13:23 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:13:23 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/29 04:13:29 WARN DAGScheduler: Broadcasting large task binary with size 1090.5 KiB\n",
      "22/11/29 04:13:30 WARN DAGScheduler: Broadcasting large task binary with size 1633.6 KiB\n",
      "22/11/29 04:13:31 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/11/29 04:13:32 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/11/29 04:13:33 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/11/29 04:13:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/11/29 04:13:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "22/11/29 04:13:34 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "22/11/29 04:13:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features',labelCol='outcome')\n",
    "\n",
    "rf_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.maxDepth, [5,10,15])\n",
    "                 .addGrid(rf.numTrees, [15,20,25])\n",
    "                 .build())\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf, \n",
    "                       estimatorParamMaps=rf_param_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       parallelism=8,\n",
    "                       numFolds=5)\n",
    "\n",
    "rf_cv_model = rf_cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "204175d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# after Cross-Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE of RandomForest Model is : 0.014993017564290159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1415:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of RandomForest Model is : 0.08411291366062913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# after Cross-Validation\n",
    "\n",
    "print(\"# after Cross-Validation\")\n",
    "\n",
    "rf_train_predictions = rf_cv_model.transform(df_train)\n",
    "print('Train MSE of RandomForest Model is :', evaluator.evaluate(rf_train_predictions))\n",
    "\n",
    "rf_test_predictions = rf_cv_model.transform(df_test)\n",
    "print('Test MSE of RandomForest Model is :', evaluator.evaluate(rf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dc1b0",
   "metadata": {},
   "source": [
    "### 4.4 Model on Tensorflow\n",
    "I built Basic regression Model (only 1 layer, which is like Linear Regression) and Deep Nueral Network(more than 3 layers)\n",
    "\n",
    "since it's a regression task, I choose MSE(mean squared error) as the target, which is also the loss I used.\n",
    "\n",
    "the dataset is devided to train(80%),validate(10%),test(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1010f858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "df_test_half, df_validate = df_test.randomSplit(weights=[0.5, 0.5], seed=2022)\n",
    "\n",
    "df_train_pandas = df_train.withColumn('features', to_array('features')).toPandas()\n",
    "df_validate_pandas = df_validate.withColumn('features', to_array('features')).toPandas()\n",
    "df_test_half_pandas = df_test_half.withColumn('features', to_array('features')).toPandas()\n",
    "\n",
    "x_train = tf.constant(np.array(df_train_pandas['features'].values.tolist()))\n",
    "y_train = tf.constant(np.array(df_train_pandas['outcome'].values.tolist()))\n",
    "\n",
    "x_validate = tf.constant(np.array(df_validate_pandas['features'].values.tolist()))\n",
    "y_validate = tf.constant(np.array(df_validate_pandas['outcome'].values.tolist()))\n",
    "\n",
    "x_test = tf.constant(np.array(df_test_half_pandas['features'].values.tolist()))\n",
    "y_test = tf.constant(np.array(df_test_half_pandas['outcome'].values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0fae9",
   "metadata": {},
   "source": [
    "#### 4.4.1 (tensorflow) Basic regression \n",
    "\n",
    "the Basic regression model only has 1 layer, and choosed the loss of 'mean_squared_error'\n",
    "\n",
    "the hyperpamameter is:\\\n",
    "learningrate = [0.01,0.1]\\\n",
    "epochs = [10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7306d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- learningrate is : 0.01\n",
      "--- epochs is : 10\n",
      "--- loss(mean_squared_error,mse) for this trial is: 3.7285938262939453\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 2.725465774536133\n",
      "\n",
      "--- learningrate is : 0.01\n",
      "--- epochs is : 100\n",
      "--- loss(mean_squared_error,mse) for this trial is: 1.0989384651184082\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 0.6596671938896179\n",
      "\n",
      "--- learningrate is : 0.1\n",
      "--- epochs is : 10\n",
      "--- loss(mean_squared_error,mse) for this trial is: 6.85516881942749\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 5.283373832702637\n",
      "\n",
      "--- learningrate is : 0.1\n",
      "--- epochs is : 100\n",
      "--- loss(mean_squared_error,mse) for this trial is: 5.850796222686768\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 2.210355043411255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "#df_transformed_pandas = df_transformed.withColumn('features', to_array('features')).toPandas()\n",
    "\n",
    "HP_learningrate = [0.01,0.1]\n",
    "HP_epochs = [10,100]\n",
    "\n",
    "learningrate = []\n",
    "epochs = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "for r in HP_learningrate:\n",
    "    for e in HP_epochs:\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(1))   \n",
    "        model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(r))\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs = e, verbose = 0,\n",
    "                            validation_data = (x_validate, y_validate))\n",
    "        loss.append(np.min(history.history[\"loss\"]))\n",
    "        val_loss.append(np.min(history.history[\"val_loss\"]))\n",
    "        learningrate.append(r)\n",
    "        epochs.append(e)\n",
    "\n",
    "\n",
    "        print('--- learningrate is :',r)\n",
    "        print('--- epochs is :',e)\n",
    "        print('--- loss(mean_squared_error,mse) for this trial is:',loss[-1])\n",
    "        print('--- val_loss(mean_squared_error,mse) for this trial is:',val_loss[-1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1e352f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 4.8586 - 29ms/epoch - 1ms/step\n",
      "\n",
      "best model has learningrate of 0.01\n",
      "best model has epochs of 100\n",
      "train loss (mean squared error, MSE) of best Model is : 0.9805044531822205\n",
      "validation loss (mean squared error, MSE) of best Model is : 0.6801302433013916\n",
      "test loss (mean squared error, MSE) of best Model is : 4.858574390411377\n"
     ]
    }
   ],
   "source": [
    "best_lr = learningrate[np.array(val_loss).argmin()]\n",
    "best_epochs = epochs[np.array(val_loss).argmin()]\n",
    "\n",
    "best_model = keras.Sequential()\n",
    "best_model.add(keras.layers.Dense(1))\n",
    "\n",
    "best_model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(best_lr))\n",
    "\n",
    "history = best_model.fit(x_train,y_train, epochs = best_epochs,validation_data=(x_validate,y_validate),verbose = 0)\n",
    "a = np.min(history.history[\"loss\"])\n",
    "b = np.min(history.history[\"val_loss\"])\n",
    "c = best_model.evaluate(x_test,y_test, verbose = 2)\n",
    "\n",
    "print()\n",
    "print('best model has learningrate of', best_lr)\n",
    "print('best model has epochs of', best_epochs)\n",
    "print(\"train loss (mean squared error, MSE) of best Model is :\",a)\n",
    "print(\"validation loss (mean squared error, MSE) of best Model is :\",b)\n",
    "print(\"test loss (mean squared error, MSE) of best Model is :\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b1bc8",
   "metadata": {},
   "source": [
    "#### 4.4.2 (tensorflow) Deep Nueral Network\n",
    "the Deep Nueral Network model has more than 3 layers, and choosed the loss of 'mean_squared_error', the activation function of 'relu',epochs = 100\n",
    "\n",
    "the hyperpamameter is:\\\n",
    "WIDTH = [10,15,20]\\\n",
    "DEPTH = [3,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee113aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Depth is : 3\n",
      "--- Width is : 10\n",
      "--- loss(mean_squared_error,mse) for this trial is: 2.1048104763031006\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.4913729429244995\n",
      "\n",
      "--- Depth is : 3\n",
      "--- Width is : 15\n",
      "--- loss(mean_squared_error,mse) for this trial is: 2.1714603900909424\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.5114251375198364\n",
      "\n",
      "--- Depth is : 3\n",
      "--- Width is : 20\n",
      "--- loss(mean_squared_error,mse) for this trial is: 2.256932258605957\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.5276556015014648\n",
      "\n",
      "--- Depth is : 5\n",
      "--- Width is : 10\n",
      "--- loss(mean_squared_error,mse) for this trial is: 2.178896903991699\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.6898306608200073\n",
      "\n",
      "--- Depth is : 5\n",
      "--- Width is : 15\n",
      "--- loss(mean_squared_error,mse) for this trial is: 1.8955179452896118\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.2388601303100586\n",
      "\n",
      "--- Depth is : 5\n",
      "--- Width is : 20\n",
      "--- loss(mean_squared_error,mse) for this trial is: 1.6829286813735962\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.2449061870574951\n",
      "\n",
      "--- Depth is : 7\n",
      "--- Width is : 10\n",
      "--- loss(mean_squared_error,mse) for this trial is: 1.9304375648498535\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.445341944694519\n",
      "\n",
      "--- Depth is : 7\n",
      "--- Width is : 15\n",
      "--- loss(mean_squared_error,mse) for this trial is: 2.2656826972961426\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.486794114112854\n",
      "\n",
      "--- Depth is : 7\n",
      "--- Width is : 20\n",
      "--- loss(mean_squared_error,mse) for this trial is: 1.683949589729309\n",
      "--- val_loss(mean_squared_error,mse) for this trial is: 1.11794114112854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "#df_transformed_pandas = df_transformed.withColumn('features', to_array('features')).toPandas()\n",
    "\n",
    "HP_WIDTH = [10,15,20]\n",
    "HP_DEPTH = [3,5,7]\n",
    "\n",
    "depth = []\n",
    "width = []\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "for dep in HP_DEPTH:\n",
    "    for wid in HP_WIDTH:\n",
    "        model = keras.Sequential()\n",
    "        for i in range(dep):\n",
    "            model.add(keras.layers.Dense(wid,activation='relu'))\n",
    "        model.add(keras.layers.Dense(1))   \n",
    "        model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs = 100, verbose = 0,\n",
    "                            validation_data = (x_validate, y_validate))\n",
    "        loss.append(np.min(history.history[\"loss\"]))\n",
    "        val_loss.append(np.min(history.history[\"val_loss\"]))\n",
    "        depth.append(dep)\n",
    "        width.append(wid)\n",
    "\n",
    "\n",
    "        print('--- Depth is :',dep)\n",
    "        print('--- Width is :',wid)\n",
    "        print('--- loss(mean_squared_error,mse) for this trial is:',loss[-1])\n",
    "        print('--- val_loss(mean_squared_error,mse) for this trial is:',val_loss[-1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bd43892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 0s - loss: 1.8554 - 30ms/epoch - 1ms/step\n",
      "\n",
      "best model has depth of 7\n",
      "best model has width of 20\n",
      "train loss (mean squared error, MSE) of best Model is : 2.519286632537842\n",
      "validation loss (mean squared error, MSE) of best Model is : 1.4494751691818237\n",
      "test loss (mean squared error, MSE) of best Model is : 1.85536527633667\n"
     ]
    }
   ],
   "source": [
    "best_wid = width[np.array(val_loss).argmin()]\n",
    "best_dep = depth[np.array(val_loss).argmin()]\n",
    "\n",
    "best_model = keras.Sequential()\n",
    "for i in range(best_dep):\n",
    "    best_model.add(keras.layers.Dense(best_wid,activation='relu'))\n",
    "best_model.add(keras.layers.Dense(1))\n",
    "\n",
    "best_model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "\n",
    "history = best_model.fit(x_train,y_train, epochs = 100,validation_data=(x_validate,y_validate),verbose = 0)\n",
    "a = np.min(history.history[\"loss\"])\n",
    "b = np.min(history.history[\"val_loss\"])\n",
    "c = best_model.evaluate(x_test,y_test, verbose = 2)\n",
    "\n",
    "print()\n",
    "print('best model has depth of', best_dep)\n",
    "print('best model has width of', best_wid)\n",
    "print(\"train loss (mean squared error, MSE) of best Model is :\",a)\n",
    "print(\"validation loss (mean squared error, MSE) of best Model is :\",b)\n",
    "print(\"test loss (mean squared error, MSE) of best Model is :\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c2143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
